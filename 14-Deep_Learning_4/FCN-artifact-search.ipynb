{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import VGG\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import scipy.misc\n",
    "import random\n",
    "import sys\n",
    "if '/opt/ros/kinetic/lib/python2.7/dist-packages' in sys.path:\n",
    "    sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')\n",
    "import cv2\n",
    "from torch.optim import lr_scheduler                                                                                                                                                                                       \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "from zipfile import ZipFile\n",
    "# import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define FCN16s model for deconvolution layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1,\n",
    "                             padding=0, output_padding=0, groups=1, bias=True, dilation=1)\n",
    "                             \n",
    "-  in_channels – the number of channels of the input signal\n",
    "-  out_channels – the number of channels generated by convolution\n",
    "-  kerner_size – size of convolution kernel\n",
    "-  stride – Convolution step size, that is, the multiple of the input to be expanded.\n",
    "-  padding – The height and width are increased by 2 * padding\n",
    "-  output_padding – the height and width are increased by padding\n",
    "-  groups – number of blocked connections from input channel to output channel\n",
    "-  bias – if bias = True, add bias\n",
    "-  dilation – spacing between elements of the convolution kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN16s(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super(FCN16s, self).__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace = True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        \n",
    "        # After the feature extraction layer of vgg, you can get the feature map. \n",
    "        # The size of the feature map after 5 max_pools are respectively\n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "        x2 = output['x2']  # size=(N, 128, x.H/4,  x.W/4)\n",
    "        x1 = output['x1']  # size=(N, 64, x.H/2,  x.W/2)\n",
    "        \n",
    "#===========FCN16s model ==========================\n",
    "        score = self.relu(self.deconv1(x5))               # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn1(score + x4)                      # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "#===========FCN16s model ==========================\n",
    "        \n",
    "#===========Please design a FCN8s model ===========\n",
    "    \n",
    "\n",
    "        \n",
    "#===========Please design a FCN8s model ===========\n",
    "\n",
    "\n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define VGG16 for convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(VGG):\n",
    "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
    "        super(VGGNet, self).__init__(make_layers(cfg[model]))\n",
    "        self.ranges = ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(models.%s(pretrained=True).state_dict())\" % model)\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
    "            del self.classifier\n",
    "\n",
    "        if show_params:\n",
    "            for name, param in self.named_parameters():\n",
    "                print(name, param.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):      \n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {\n",
    "    'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
    "    'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
    "    'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
    "    'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
    "}\n",
    "\n",
    "# cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "cfg = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "epochs     = 25  #500\n",
    "lr         = 1e-4\n",
    "momentum   = 0\n",
    "w_decay    = 1e-5\n",
    "step_size  = 50\n",
    "gamma      = 0.5\n",
    "model_use  = \"subt_model\" # \"subt_model\"\n",
    "n_class = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading dataset.\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://drive.google.com/u/1/uc?id=1ORB2iXh0Os-64WqIrE7rWJBsEvQsFTVs&export=download\"\n",
    "dataset_name = \"data\"\n",
    "if not os.path.isdir(dataset_name):\n",
    "    gdown.download(dataset_url, output=dataset_name + '.zip', quiet=False)\n",
    "    zip1 = ZipFile(dataset_name + '.zip')\n",
    "    zip1.extractall(dataset_name)\n",
    "    zip1.close()\n",
    "    os.remove(\"data.zip\")\n",
    "\n",
    "print(\"Finished downloading dataset.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define path, directory trainning environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/sis_2020/.cache/torch/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b45fe7810d400aabfa807d69d73cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finish cuda loading, time elapsed 2.0601770877838135\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "FullPath = os.getcwd()\n",
    "data_dir  = os.path.join(FullPath + \"/data/subt\")\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Data not found!\")\n",
    "    \n",
    "# create dir for model\n",
    "model_dir = os.path.join(FullPath + \"/models\", model_use)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# create dir for score\n",
    "score_dir = os.path.join(FullPath + \"/scores\", model_use)\n",
    "if not os.path.exists(score_dir):\n",
    "    os.makedirs(score_dir)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "num_gpu = list(range(torch.cuda.device_count()))\n",
    "\n",
    "vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n",
    "fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)\n",
    "\n",
    "if use_gpu:\n",
    "    ts = time.time()\n",
    "    vgg_model = vgg_model.cuda()\n",
    "    fcn_model = fcn_model.cuda()\n",
    "    fcn_model = nn.DataParallel(fcn_model, device_ids=num_gpu)\n",
    "    print(\"Finish cuda loading, time elapsed {}\".format(time.time() - ts))\n",
    "else:\n",
    "#     nn.DataParallel(fcn_model)\n",
    "    print(\"Use CPU to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): FCN16s(\n",
      "    (pretrained_net): VGGNet(\n",
      "      (features): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (8): ReLU(inplace=True)\n",
      "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (11): ReLU(inplace=True)\n",
      "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (13): ReLU(inplace=True)\n",
      "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (15): ReLU(inplace=True)\n",
      "        (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (18): ReLU(inplace=True)\n",
      "        (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (20): ReLU(inplace=True)\n",
      "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (22): ReLU(inplace=True)\n",
      "        (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (25): ReLU(inplace=True)\n",
      "        (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (27): ReLU(inplace=True)\n",
      "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (29): ReLU(inplace=True)\n",
      "        (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (deconv1): ConvTranspose2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (deconv2): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (deconv3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (deconv5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (classifier): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(fcn_model)\n",
    "params = list(fcn_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class\n",
    "-------------\n",
    "\n",
    "``torch.utils.data.Dataset`` is an abstract class representing a\n",
    "dataset.  \n",
    "Your custom dataset should inherit ``Dataset`` and override the following\n",
    "methods:\n",
    "\n",
    "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
    "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
    "   be used to get $i$\\ th sample\n",
    "\n",
    "Let's create a dataset class for our face landmarks dataset.  \n",
    "We will read the csv in ``__init__`` but leave the reading of images to\n",
    "``__getitem__``.   \n",
    "This is memory efficient because all the images are not\n",
    "stored in the memory at once but read as required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "means     = np.array([103.939, 116.779, 123.68]) / 255. # mean of three channels in the order of BGR\n",
    "h, w      = 480, 640\n",
    "val_h     = h\n",
    "val_w     = w\n",
    "\n",
    "class product_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, phase, n_class=n_class, flip_rate=0.):\n",
    "        data_dir = os.path.join(root, phase)\n",
    "        self.rgb_list = os.listdir(os.path.join(data_dir, 'images'))\n",
    "        _list = self.rgb_list\n",
    "        self.label_list = []\n",
    "        for i in range(len(self.rgb_list)):\n",
    "            self.label_list.append(_list[i].split(\".\")[0] + \".png\")\n",
    "\n",
    "        self.rgb_dir = os.path.join(data_dir, 'images')\n",
    "        self.label_dir = os.path.join(data_dir, 'masks')\n",
    "        self.means     = means\n",
    "        self.n_class   = n_class\n",
    "        self.flip_rate = flip_rate\n",
    "        if phase == 'train':\n",
    "            self.flip_rate = 0.5\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.rgb_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.rgb_list)        \n",
    "        img = cv2.imread(os.path.join(self.rgb_dir, self.rgb_list[idx]),cv2.IMREAD_UNCHANGED)\n",
    "        label = cv2.imread(os.path.join(self.label_dir, self.label_list[idx]), cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #===SubT label error===backpack = 38, survivor = 75, vent = 113, phone = 14\n",
    "        label[label == 38] = 1\n",
    "        label[label == 75] = 2\n",
    "        label[label == 113] = 3\n",
    "        label[label == 14] = 4  \n",
    "        #===SubT error===       \n",
    "        \n",
    "        img = cv2.resize(img, (640, 480), interpolation=cv2.INTER_CUBIC)\n",
    "        label = cv2.resize(label, (640, 480), interpolation=cv2.INTER_CUBIC)\n",
    "   \n",
    "        origin_img = img\n",
    "        if random.random() < self.flip_rate:\n",
    "            img   = np.fliplr(img)\n",
    "            label = np.fliplr(label)\n",
    "\n",
    "        # reduce mean\n",
    "        img = img[:, :, ::-1]  # switch to BGR\n",
    "        \n",
    "        img = np.transpose(img, (2, 0, 1)) / 255.\n",
    "        img[0] -= self.means[0]\n",
    "        img[1] -= self.means[1]\n",
    "        img[2] -= self.means[2]\n",
    "\n",
    "        # convert to tensor\n",
    "        img = torch.from_numpy(img.copy()).float()\n",
    "        label = torch.from_numpy(label.copy()).long()\n",
    "\n",
    "        # create one-hot encoding\n",
    "        h, w = label.size()\n",
    "        target = torch.zeros(self.n_class, h, w)\n",
    "        \n",
    "        for i in range(n_class):\n",
    "            target[i][label == i] = 1\n",
    "        \n",
    "#         target[0][label == 0] = 1\n",
    "#         print(np.unique(label))\n",
    "        \n",
    " \n",
    "        sample = {'X': img, 'Y': target, 'l': label, 'origin': origin_img}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial dataloader for trainning and validation\n",
    "train_data = product_dataset(data_dir, phase = 'train')\n",
    "val_data   = product_dataset(data_dir, phase = 'test', flip_rate = 0)\n",
    "dataloader = DataLoader(train_data, batch_size = batch_size, shuffle=True, num_workers = 0)\n",
    "val_loader = DataLoader(val_data, batch_size = 1, num_workers = 0)\n",
    "\n",
    "dataiter = iter(val_loader)\n",
    "\n",
    "# define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.RMSprop(fcn_model.parameters(), lr = lr, momentum = momentum, weight_decay = w_decay)\n",
    "# decay LR by a factor of 0.5 every step_size = 50 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size = step_size, gamma = gamma)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        fcn_model.train()\n",
    "        scheduler.step()\n",
    "        configs    = \"FCNs_{}_batch{}_epoch{}_RMSprop_lr{}\"\\\n",
    "            .format(model_use, batch_size, epoch, lr)\n",
    "        model_path = os.path.join(model_dir, configs)\n",
    "        \n",
    "        ts = time.time()\n",
    "        for iter, batch in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs = Variable(batch['X'].cuda())\n",
    "                labels = Variable(batch['Y'].cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(batch['X']), Variable(batch['Y'])\n",
    "\n",
    "            outputs = fcn_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iter % 1 == 0:\n",
    "                print(\"epoch{}, iter{}, loss: {}\".format(epoch+1, iter, loss.item()))\n",
    "        \n",
    "        print(\"Finish epoch {}, time elapsed {}\".format(epoch+1, time.time() - ts))\n",
    "        if epoch % 1 == 0:\n",
    "            torch.save(fcn_model.state_dict(),model_path + '.pkl')\n",
    "\n",
    "        val(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):\n",
    "    fcn_model.eval()\n",
    "    TP = np.zeros(n_class-1)\n",
    "    FN = np.zeros(n_class-1)\n",
    "    FP = np.zeros(n_class-1)\n",
    "    total_ious = []\n",
    "    pixel_accs = []\n",
    "    for iter, batch in enumerate(val_loader):\n",
    "        if use_gpu:\n",
    "            inputs = Variable(batch['X'].cuda())\n",
    "        else:\n",
    "            inputs = Variable(batch['X'])\n",
    "\n",
    "        output = fcn_model(inputs)\n",
    "        output = output.data.cpu().numpy()\n",
    "\n",
    "        N, _, h, w = output.shape\n",
    "        pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis=1).reshape(N, h, w)\n",
    "\n",
    "        target = batch['l'].cpu().numpy().reshape(N, h, w)\n",
    "        for p, t in zip(pred, target):\n",
    "            pixel_accs.append(pixel_acc(p, t))\n",
    "            _TP, _FN, _FP =  analysis(p, t, h, w)\n",
    "            TP += _TP[1:n_class]\n",
    "            FN += _FN[1:n_class]\n",
    "            FP += _FP[1:n_class]\n",
    "            \n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    ious = TP / (TP + FN + FP)\n",
    "    fscore = 2*TP / (2*TP + FN + FP)\n",
    "    total_ious = np.array(total_ious).T  # n_class * val_len\n",
    "    pixel_accs = np.array(pixel_accs).mean()\n",
    "    \n",
    "#     print(\"epoch{}, pix_acc: {}, meanIoU: {}, IoUs: {}, recall: {}, precision: {}, fscore: {}\"\\\n",
    "#           .format(epoch, pixel_accs, np.nanmean(ious), ious, recall, precision, fscore))\n",
    "    \n",
    "    f1 = open(score_dir + \"/cls_acc_log.txt\",\"a+\")\n",
    "    f1.write('epoch:'+ str(epoch) + ', pix_acc: ' + str(pixel_accs) + '\\n' )\n",
    "    f2 = open(score_dir + \"/cls_iou_log.txt\",\"a+\")\n",
    "    f2.write('epoch:'+ str(epoch) + ', class ious: ' + str(ious) + '\\n' )\n",
    "    f3 = open(score_dir + \"/mean_iou_log.txt\",\"a+\")\n",
    "    f3.write('epoch:'+ str(epoch) + ', mean IoU: ' + str(np.nanmean(ious)) + '\\n' ) \n",
    "    f4 = open(score_dir + \"/recall_log.txt\",\"a+\")\n",
    "    f4.write('epoch:'+ str(epoch) + ', class recall: ' + str(recall) + '\\n' )\n",
    "    f5 = open(score_dir + \"/precision_log.txt\",\"a+\")\n",
    "    f5.write('epoch:'+ str(epoch) + ', class precision: ' + str(precision) + '\\n' )    \n",
    "    f6 = open(score_dir + \"/fscore_log.txt\",\"a+\")\n",
    "    f6.write('epoch:'+ str(epoch) + ', class fscore: ' + str(fscore) + '\\n' )  \n",
    "    \n",
    "\n",
    "def analysis(pred, target, h, w):\n",
    "    # TP, FN, FP, TN\n",
    "    TP = np.zeros(n_class)\n",
    "    FN = np.zeros(n_class)\n",
    "    FP = np.zeros(n_class)\n",
    "\n",
    "    target = target.reshape(h * w)\n",
    "    pred = pred.reshape(h * w)\n",
    "\n",
    "    con_matrix = confusion_matrix(target, pred,labels = np.arange(0,n_class,1))\n",
    "    con_matrix[0][0] = 0\n",
    "    for i in range(0, n_class):\n",
    "        for j in range(0, n_class):\n",
    "            if i == j:\n",
    "                TP[i] += con_matrix[i][j]\n",
    "            if i != j:\n",
    "                FP[j] += con_matrix[i][j]\n",
    "                FN[i] += con_matrix[i][j]\n",
    "    return TP, FN, FP\n",
    "                \n",
    "def pixel_acc(pred, target):\n",
    "    correct = (pred == target).sum()\n",
    "    total   = (target == target).sum()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1, iter0, loss: 0.7067620158195496\n",
      "epoch1, iter1, loss: 0.690791130065918\n",
      "epoch1, iter2, loss: 0.6778188943862915\n",
      "epoch1, iter3, loss: 0.6861029267311096\n",
      "epoch1, iter4, loss: 0.6738225817680359\n",
      "Finish epoch 1, time elapsed 1.446009874343872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch2, iter0, loss: 0.6711183190345764\n",
      "epoch2, iter1, loss: 0.6756827235221863\n",
      "epoch2, iter2, loss: 0.6669906973838806\n",
      "epoch2, iter3, loss: 0.6681455373764038\n",
      "epoch2, iter4, loss: 0.6657367944717407\n",
      "Finish epoch 2, time elapsed 1.2587523460388184\n",
      "epoch3, iter0, loss: 0.674485981464386\n",
      "epoch3, iter1, loss: 0.6673469543457031\n",
      "epoch3, iter2, loss: 0.6649817824363708\n",
      "epoch3, iter3, loss: 0.6705513596534729\n",
      "epoch3, iter4, loss: 0.6629231572151184\n",
      "Finish epoch 3, time elapsed 1.2823331356048584\n",
      "epoch4, iter0, loss: 0.6684812307357788\n",
      "epoch4, iter1, loss: 0.664252519607544\n",
      "epoch4, iter2, loss: 0.6612322926521301\n",
      "epoch4, iter3, loss: 0.664825439453125\n",
      "epoch4, iter4, loss: 0.6613366007804871\n",
      "Finish epoch 4, time elapsed 1.2723348140716553\n",
      "epoch5, iter0, loss: 0.6661473512649536\n",
      "epoch5, iter1, loss: 0.6602115631103516\n",
      "epoch5, iter2, loss: 0.6603005528450012\n",
      "epoch5, iter3, loss: 0.6648879051208496\n",
      "epoch5, iter4, loss: 0.6598577499389648\n",
      "Finish epoch 5, time elapsed 1.2744026184082031\n",
      "epoch6, iter0, loss: 0.6596725583076477\n",
      "epoch6, iter1, loss: 0.6610292792320251\n",
      "epoch6, iter2, loss: 0.6650545001029968\n",
      "epoch6, iter3, loss: 0.6588420271873474\n",
      "epoch6, iter4, loss: 0.6600194573402405\n",
      "Finish epoch 6, time elapsed 1.3112168312072754\n",
      "epoch7, iter0, loss: 0.6601612567901611\n",
      "epoch7, iter1, loss: 0.6605263948440552\n",
      "epoch7, iter2, loss: 0.6597527265548706\n",
      "epoch7, iter3, loss: 0.6600668430328369\n",
      "epoch7, iter4, loss: 0.6577115058898926\n",
      "Finish epoch 7, time elapsed 1.2455339431762695\n",
      "epoch8, iter0, loss: 0.657922089099884\n",
      "epoch8, iter1, loss: 0.6574198603630066\n",
      "epoch8, iter2, loss: 0.6591795682907104\n",
      "epoch8, iter3, loss: 0.6572678089141846\n",
      "epoch8, iter4, loss: 0.6552811861038208\n",
      "Finish epoch 8, time elapsed 1.2706420421600342\n",
      "epoch9, iter0, loss: 0.6552895903587341\n",
      "epoch9, iter1, loss: 0.6575098633766174\n",
      "epoch9, iter2, loss: 0.6562703251838684\n",
      "epoch9, iter3, loss: 0.6561470627784729\n",
      "epoch9, iter4, loss: 0.6555718779563904\n",
      "Finish epoch 9, time elapsed 1.2654485702514648\n",
      "epoch10, iter0, loss: 0.6546564698219299\n",
      "epoch10, iter1, loss: 0.6578640341758728\n",
      "epoch10, iter2, loss: 0.6535230875015259\n",
      "epoch10, iter3, loss: 0.6528459191322327\n",
      "epoch10, iter4, loss: 0.6543977856636047\n",
      "Finish epoch 10, time elapsed 1.2479474544525146\n",
      "epoch11, iter0, loss: 0.6507239937782288\n",
      "epoch11, iter1, loss: 0.6540107727050781\n",
      "epoch11, iter2, loss: 0.6557327508926392\n",
      "epoch11, iter3, loss: 0.6533597111701965\n",
      "epoch11, iter4, loss: 0.6554316282272339\n",
      "Finish epoch 11, time elapsed 1.2603986263275146\n",
      "epoch12, iter0, loss: 0.651614785194397\n",
      "epoch12, iter1, loss: 0.6536023020744324\n",
      "epoch12, iter2, loss: 0.6520599126815796\n",
      "epoch12, iter3, loss: 0.6526077389717102\n",
      "epoch12, iter4, loss: 0.6487833261489868\n",
      "Finish epoch 12, time elapsed 1.2735717296600342\n",
      "epoch13, iter0, loss: 0.6461586356163025\n",
      "epoch13, iter1, loss: 0.6516013145446777\n",
      "epoch13, iter2, loss: 0.6546692848205566\n",
      "epoch13, iter3, loss: 0.653879702091217\n",
      "epoch13, iter4, loss: 0.6510622501373291\n",
      "Finish epoch 13, time elapsed 1.2773873805999756\n",
      "epoch14, iter0, loss: 0.6459020376205444\n",
      "epoch14, iter1, loss: 0.6444500088691711\n",
      "epoch14, iter2, loss: 0.6500915884971619\n",
      "epoch14, iter3, loss: 0.6440187096595764\n",
      "epoch14, iter4, loss: 0.6443120241165161\n",
      "Finish epoch 14, time elapsed 1.2622053623199463\n",
      "epoch15, iter0, loss: 0.6489200592041016\n",
      "epoch15, iter1, loss: 0.6423259377479553\n",
      "epoch15, iter2, loss: 0.6538424491882324\n",
      "epoch15, iter3, loss: 0.642545223236084\n",
      "epoch15, iter4, loss: 0.6376064419746399\n",
      "Finish epoch 15, time elapsed 1.2546327114105225\n",
      "epoch16, iter0, loss: 0.6386983394622803\n",
      "epoch16, iter1, loss: 0.6385749578475952\n",
      "epoch16, iter2, loss: 0.6489244103431702\n",
      "epoch16, iter3, loss: 0.6308435797691345\n",
      "epoch16, iter4, loss: 0.6558273434638977\n",
      "Finish epoch 16, time elapsed 1.2610299587249756\n",
      "epoch17, iter0, loss: 0.6385524272918701\n",
      "epoch17, iter1, loss: 0.6490375995635986\n",
      "epoch17, iter2, loss: 0.6321858763694763\n",
      "epoch17, iter3, loss: 0.6230230331420898\n",
      "epoch17, iter4, loss: 0.6451057195663452\n",
      "Finish epoch 17, time elapsed 1.3487226963043213\n",
      "epoch18, iter0, loss: 0.647076427936554\n",
      "epoch18, iter1, loss: 0.6394869685173035\n",
      "epoch18, iter2, loss: 0.6424544453620911\n",
      "epoch18, iter3, loss: 0.642693281173706\n",
      "epoch18, iter4, loss: 0.6245102882385254\n",
      "Finish epoch 18, time elapsed 1.263456106185913\n",
      "epoch19, iter0, loss: 0.6535390615463257\n",
      "epoch19, iter1, loss: 0.6453946232795715\n",
      "epoch19, iter2, loss: 0.6325247287750244\n",
      "epoch19, iter3, loss: 0.6333178281784058\n",
      "epoch19, iter4, loss: 0.6203854084014893\n",
      "Finish epoch 19, time elapsed 1.283919095993042\n",
      "epoch20, iter0, loss: 0.6463203430175781\n",
      "epoch20, iter1, loss: 0.6231167316436768\n",
      "epoch20, iter2, loss: 0.6325950622558594\n",
      "epoch20, iter3, loss: 0.6257412433624268\n",
      "epoch20, iter4, loss: 0.6581978797912598\n",
      "Finish epoch 20, time elapsed 1.2608532905578613\n",
      "epoch21, iter0, loss: 0.6234436631202698\n",
      "epoch21, iter1, loss: 0.6490570902824402\n",
      "epoch21, iter2, loss: 0.6239354610443115\n",
      "epoch21, iter3, loss: 0.6417282819747925\n",
      "epoch21, iter4, loss: 0.6178262829780579\n",
      "Finish epoch 21, time elapsed 1.2769017219543457\n",
      "epoch22, iter0, loss: 0.6441683173179626\n",
      "epoch22, iter1, loss: 0.6203951835632324\n",
      "epoch22, iter2, loss: 0.6293720006942749\n",
      "epoch22, iter3, loss: 0.6367760896682739\n",
      "epoch22, iter4, loss: 0.6252380013465881\n",
      "Finish epoch 22, time elapsed 1.3529999256134033\n",
      "epoch23, iter0, loss: 0.6426386833190918\n",
      "epoch23, iter1, loss: 0.6264878511428833\n",
      "epoch23, iter2, loss: 0.6206480264663696\n",
      "epoch23, iter3, loss: 0.638539731502533\n",
      "epoch23, iter4, loss: 0.609565258026123\n",
      "Finish epoch 23, time elapsed 1.2803874015808105\n",
      "epoch24, iter0, loss: 0.6331558227539062\n",
      "epoch24, iter1, loss: 0.6058319211006165\n",
      "epoch24, iter2, loss: 0.6425120234489441\n",
      "epoch24, iter3, loss: 0.6197508573532104\n",
      "epoch24, iter4, loss: 0.6175419092178345\n",
      "Finish epoch 24, time elapsed 1.2697741985321045\n",
      "epoch25, iter0, loss: 0.6151137351989746\n",
      "epoch25, iter1, loss: 0.63644939661026\n",
      "epoch25, iter2, loss: 0.6352874040603638\n",
      "epoch25, iter3, loss: 0.6034576892852783\n",
      "epoch25, iter4, loss: 0.6180036067962646\n",
      "Finish epoch 25, time elapsed 1.3114218711853027\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model_name):\n",
    "    \n",
    "    # load pretrain models\n",
    "              \n",
    "    vgg_model = VGGNet(requires_grad=True, remove_fc=True)\n",
    "    fcn_model = FCN16s(pretrained_net=vgg_model, n_class=n_class)\n",
    "    fcn_model = nn.DataParallel(fcn_model)      \n",
    "    \n",
    "    state_dict = torch.load(os.path.join(model_dir, model_name), map_location='cpu')\n",
    "    fcn_model.load_state_dict(state_dict)\n",
    "    \n",
    "    batch = dataiter.next()\n",
    "    if use_gpu:\n",
    "        inputs = Variable(batch['X'].cuda())\n",
    "    else:\n",
    "        inputs = Variable(batch['X'])\n",
    "    img    = batch['origin'] \n",
    "    label  = batch['l']\n",
    "    \n",
    "    inputs = Variable(batch['X'])\n",
    "    output = fcn_model(inputs)\n",
    "    output = output.data.cpu().numpy()\n",
    "\n",
    "    N, _, h, w = output.shape\n",
    "    pred = output.transpose(0, 2, 3, 1).reshape(-1, n_class).argmax(axis = 1).reshape(N, h, w)\n",
    "\n",
    "    # show images\n",
    "    plt.figure(figsize = (10, 12))\n",
    "    img = img.numpy()\n",
    "    for i in range(N):\n",
    "        img[i] = cv2.cvtColor(img[i], cv2.COLOR_BGR2RGB)\n",
    "        plt.subplot(N, 3, i*3 + 1)\n",
    "        plt.title(\"origin_img\")\n",
    "        plt.imshow(img[i])\n",
    "        #print(np.unique(_img[i]))\n",
    "\n",
    "        plt.subplot(N, 3, i*3 + 2)\n",
    "        plt.title(\"label_img\")\n",
    "        plt.imshow(label[i],cmap = \"nipy_spectral\",vmin = 0, vmax = n_class - 1)\n",
    "\n",
    "        plt.subplot(N, 3, i*3 + 3)\n",
    "        plt.title(\"prediction\")\n",
    "        plt.imshow(pred[i],cmap = \"nipy_spectral\",vmin = 0, vmax = n_class - 1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ac1376d2afbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodels_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"FCNs_subt_model_batch8_epoch200_RMSprop_lr0.0001.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"models/subt_model/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodels_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished downloading models.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gdown' is not defined"
     ]
    }
   ],
   "source": [
    "models_url = \"https://drive.google.com/u/1/uc?id=1IQI4Rvr6fYvvRuWPVKc23c4fioB1USKa&export=download\"\n",
    "models_name = \"FCNs_subt_model_batch8_epoch200_RMSprop_lr0.0001.pkl\"\n",
    "if not os.path.isfile(models_name):\n",
    "    gdown.download(models_url, output=\"models/subt_model/\" + models_name, quiet=False)\n",
    "\n",
    "print(\"Finished downloading models.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(\"FCNs_subt_model_batch8_epoch200_RMSprop_lr0.0001.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to use the model you just trained to predict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd models/subt_model\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction(\"FCNs_subt_model_batch3_epoch24_RMSprop_lr0.0001.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpack = 1, survivor = 2, vent = 3, phone = 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
